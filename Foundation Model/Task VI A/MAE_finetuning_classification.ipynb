{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Masked Autoencoder\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, img_size=64, hidden_dim=256, mask_ratio=0.75):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, hidden_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = (torch.rand_like(x) > self.mask_ratio).float()\n",
    "        masked_x = x * mask\n",
    "        encoded = self.encoder(masked_x)\n",
    "        reconstructed = self.decoder(encoded)\n",
    "        return reconstructed, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained MAE model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved MAE model\n",
    "mae_finetune = MaskedAutoencoder().to(device)\n",
    "mae_finetune.load_state_dict(torch.load(\"mae_pretrained.pth\"))\n",
    "\n",
    "print(\"Pretrained MAE model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define dataset directories for each class\n",
    "dataset_dirs = {\n",
    "    \"axion\": \"dataset/Dataset/axion\",\n",
    "    \"cdm\": \"dataset/Dataset/cdm\",\n",
    "    \"no_sub\": \"dataset/Dataset/no_sub\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LensDatasetWithLabels(Dataset):\n",
    "    def __init__(self, dataset_dirs, transform=None):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_labels = {\"axion\": 0, \"cdm\": 1, \"no_sub\": 2}\n",
    "\n",
    "        # Gather file paths and corresponding labels\n",
    "        for class_name, dir_path in dataset_dirs.items():\n",
    "            label = self.class_labels[class_name]\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith('.npy'):\n",
    "                    file_path = os.path.join(dir_path, file)\n",
    "                    self.data.append(file_path)\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        img = np.load(img_path, allow_pickle=True)\n",
    "        \n",
    "        # If the loaded array is of object type, extract the image from the first element.\n",
    "        if isinstance(img, np.ndarray) and img.dtype == object:\n",
    "            img = img[0]\n",
    "        \n",
    "        # Ensure the image is a float32 numpy array\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        \n",
    "        # If image has more than 2 dimensions, extract the first channel (assume grayscale)\n",
    "        if img.ndim > 2:\n",
    "            img = img[:, :, 0]\n",
    "        \n",
    "        # Apply transforms (ToTensor will convert (H,W) to (1,H,W) for grayscale)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the transformation pipeline\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Create the full dataset\n",
    "fine_tune_dataset = LensDatasetWithLabels(dataset_dirs, transform=data_transform)\n",
    "\n",
    "# Split the dataset: 90% for training, 10% for validation\n",
    "train_size = int(0.9 * len(fine_tune_dataset))\n",
    "val_size = len(fine_tune_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(fine_tune_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch:\n",
      "Batch shape: torch.Size([32, 1, 64, 64])\n",
      "Labels: tensor([2, 2, 0, 1, 0, 0, 2, 2, 0, 1, 2, 2, 1, 2, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2,\n",
      "        0, 2, 0, 1, 1, 2, 1, 1])\n",
      "Validation batch:\n",
      "Batch shape: torch.Size([32, 1, 64, 64])\n",
      "Labels: tensor([1, 2, 1, 2, 0, 1, 1, 2, 0, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1,\n",
      "        1, 1, 1, 1, 1, 2, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the DataLoaders by fetching one batch from each\n",
    "print(\"Training batch:\")\n",
    "for images, labels in train_loader:\n",
    "    print(\"Batch shape:\", images.shape)  # Expected: (batch_size, 1, H, W)\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n",
    "\n",
    "print(\"Validation batch:\")\n",
    "for images, labels in val_loader:\n",
    "    print(\"Batch shape:\", images.shape)  # Expected: (batch_size, 1, H, W)\n",
    "    print(\"Labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Use the encoder from the pre-trained MAE model\n",
    "pretrained_encoder = mae_finetune.encoder  # This is the pre-trained encoder\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder, hidden_dim=256, num_classes=3):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Adaptive average pooling to reduce spatial dimensions\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using the pre-trained encoder\n",
    "        features = self.encoder(x)  # Expected shape: (batch_size, hidden_dim, H, W)\n",
    "        pooled = self.pool(features)  # Shape: (batch_size, hidden_dim, 1, 1)\n",
    "        pooled = pooled.view(pooled.size(0), -1)  # Flatten to (batch_size, hidden_dim)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "# Create classifier model and move it to GPU (cuda)\n",
    "classifier = Classifier(pretrained_encoder).to(device)\n",
    "\n",
    "# Display the classifier model summary\n",
    "print(classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 1.1019, Train Acc: 0.3395 | Val Loss: 1.0970, Val Acc: 0.3469\n",
      "Epoch [2/20] Train Loss: 1.0988, Train Acc: 0.3445 | Val Loss: 1.0995, Val Acc: 0.3349\n",
      "Epoch [3/20] Train Loss: 1.0959, Train Acc: 0.3626 | Val Loss: 1.0937, Val Acc: 0.3415\n",
      "Epoch [4/20] Train Loss: 1.0922, Train Acc: 0.3765 | Val Loss: 1.0888, Val Acc: 0.4396\n",
      "Epoch [5/20] Train Loss: 1.0871, Train Acc: 0.4056 | Val Loss: 1.0839, Val Acc: 0.4530\n",
      "Epoch [6/20] Train Loss: 1.0802, Train Acc: 0.4277 | Val Loss: 1.0739, Val Acc: 0.5849\n",
      "Epoch [7/20] Train Loss: 1.0712, Train Acc: 0.4622 | Val Loss: 1.0652, Val Acc: 0.4405\n",
      "Epoch [8/20] Train Loss: 1.0604, Train Acc: 0.4814 | Val Loss: 1.0507, Val Acc: 0.4983\n",
      "Epoch [9/20] Train Loss: 1.0465, Train Acc: 0.5079 | Val Loss: 1.0365, Val Acc: 0.5163\n",
      "Epoch [10/20] Train Loss: 1.0329, Train Acc: 0.5285 | Val Loss: 1.0260, Val Acc: 0.5038\n",
      "Epoch [11/20] Train Loss: 1.0175, Train Acc: 0.5403 | Val Loss: 1.0202, Val Acc: 0.5739\n",
      "Epoch [12/20] Train Loss: 1.0023, Train Acc: 0.5499 | Val Loss: 0.9935, Val Acc: 0.5493\n",
      "Epoch [13/20] Train Loss: 0.9858, Train Acc: 0.5672 | Val Loss: 0.9751, Val Acc: 0.6566\n",
      "Epoch [14/20] Train Loss: 0.9701, Train Acc: 0.5724 | Val Loss: 0.9686, Val Acc: 0.5406\n",
      "Epoch [15/20] Train Loss: 0.9535, Train Acc: 0.5887 | Val Loss: 0.9517, Val Acc: 0.5329\n",
      "Epoch [16/20] Train Loss: 0.9398, Train Acc: 0.5880 | Val Loss: 0.9346, Val Acc: 0.6517\n",
      "Epoch [17/20] Train Loss: 0.9245, Train Acc: 0.5982 | Val Loss: 0.9126, Val Acc: 0.6422\n",
      "Epoch [18/20] Train Loss: 0.9120, Train Acc: 0.6013 | Val Loss: 0.9005, Val Acc: 0.5647\n",
      "Epoch [19/20] Train Loss: 0.8979, Train Acc: 0.6118 | Val Loss: 0.8908, Val Acc: 0.6029\n",
      "Epoch [20/20] Train Loss: 0.8862, Train Acc: 0.6109 | Val Loss: 0.8790, Val Acc: 0.5644\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming classifier, train_loader, and val_loader are already defined\n",
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Validation Phase\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = classifier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), \"MAE_classifier_model_65.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
